"""Custom OllamaLLM wrapper that integrates prompt-xml-strategies OllamaClient with LangChain."""

import asyncio
import sys
from typing import Any, Dict, List, Optional, Union
from pathlib import Path

from langchain_core.language_models.base import BaseLanguageModel
from langchain_core.language_models.llms import LLM
from langchain_core.outputs import LLMResult, Generation
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from pydantic import Field

from .standalone_ollama import StandaloneOllamaClient as OllamaClient, LLMError


class OllamaLLM(LLM):
    """Custom Ollama LLM using prompt-xml-strategies OllamaClient."""

    base_url: str = Field(default="http://localhost:11434", description="Ollama server URL")
    model: str = Field(default="llama3.2", description="Model name to use")
    temperature: float = Field(default=0.7, description="Sampling temperature")
    max_tokens: Optional[int] = Field(default=None, description="Maximum tokens to generate")
    top_p: float = Field(default=0.9, description="Nucleus sampling parameter")
    top_k: int = Field(default=40, description="Top-k sampling parameter")
    repeat_penalty: float = Field(default=1.1, description="Repetition penalty")
    seed: Optional[int] = Field(default=None, description="Random seed")
    keep_alive: Optional[Union[int, str]] = Field(default=None, description="Keep model alive duration")
    format: Optional[str] = Field(default=None, description="Response format (e.g., 'json')")
    ollama_client: Optional[OllamaClient] = Field(default=None, exclude=True, description="OllamaClient instance")

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        if self.ollama_client is None:
            self.ollama_client = OllamaClient(base_url=self.base_url)

    @property
    def _llm_type(self) -> str:
        """Return identifier of llm type."""
        return "ollama_custom"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Call the Ollama model and return the output.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
            **kwargs: Additional keyword arguments.

        Returns:
            The string generated by the model.
        """
        try:
            # Merge instance parameters with call-time parameters
            generation_params = {
                "model": kwargs.get("model", self.model),
                "temperature": kwargs.get("temperature", self.temperature),
                "max_tokens": kwargs.get("max_tokens", self.max_tokens),
                "top_p": kwargs.get("top_p", self.top_p),
                "top_k": kwargs.get("top_k", self.top_k),
                "repeat_penalty": kwargs.get("repeat_penalty", self.repeat_penalty),
                "seed": kwargs.get("seed", self.seed),
                "keep_alive": kwargs.get("keep_alive", self.keep_alive),
                "format": kwargs.get("format", self.format),
                "stream": False,  # Use non-streaming for synchronous call
            }

            # Remove None values
            generation_params = {k: v for k, v in generation_params.items() if v is not None}

            if run_manager:
                run_manager.on_llm_start({"name": self._llm_type}, [prompt])

            response = self.ollama_client.generate_response(
                prompt=prompt,
                **generation_params
            )

            if run_manager:
                run_manager.on_llm_end(LLMResult(generations=[[Generation(text=response)]]))

            return response

        except LLMError as e:
            if run_manager:
                run_manager.on_llm_error(e)
            raise e
        except Exception as e:
            if run_manager:
                run_manager.on_llm_error(e)
            raise LLMError(f"Unexpected error in OllamaLLM: {str(e)}")

    async def _acall(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """Async call to the Ollama model.

        Args:
            prompt: The prompt to pass into the model.
            stop: Optional list of stop words to use when generating.
            run_manager: Optional callback manager.
            **kwargs: Additional keyword arguments.

        Returns:
            The string generated by the model.
        """
        # For now, we'll run the synchronous call in a thread pool
        # since the OllamaClient doesn't have native async support
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            lambda: self._call(prompt, stop, run_manager, **kwargs)
        )

    def chat_response(
        self,
        messages: List[Dict[str, str]],
        **kwargs: Any,
    ) -> str:
        """Generate a chat response using the chat endpoint.

        Args:
            messages: List of messages with 'role' and 'content' keys.
            **kwargs: Additional parameters for generation.

        Returns:
            The chat response string.
        """
        try:
            generation_params = {
                "model": kwargs.get("model", self.model),
                "temperature": kwargs.get("temperature", self.temperature),
                "max_tokens": kwargs.get("max_tokens", self.max_tokens),
                "top_p": kwargs.get("top_p", self.top_p),
                "top_k": kwargs.get("top_k", self.top_k),
                "repeat_penalty": kwargs.get("repeat_penalty", self.repeat_penalty),
                "seed": kwargs.get("seed", self.seed),
                "keep_alive": kwargs.get("keep_alive", self.keep_alive),
                "format": kwargs.get("format", self.format),
                "stream": False,
            }

            # Remove None values
            generation_params = {k: v for k, v in generation_params.items() if v is not None}

            return self.ollama_client.chat_response(
                messages=messages,
                **generation_params
            )

        except LLMError as e:
            raise e
        except Exception as e:
            raise LLMError(f"Unexpected error in chat_response: {str(e)}")

    def get_available_models(self) -> List[str]:
        """Get list of available models from Ollama server.

        Returns:
            List of available model names.
        """
        try:
            return self.ollama_client.get_available_models()
        except LLMError as e:
            raise e
        except Exception as e:
            raise LLMError(f"Error getting available models: {str(e)}")

    def validate_connection(self) -> bool:
        """Validate connection to Ollama server.

        Returns:
            True if connection is successful.
        """
        try:
            return self.ollama_client.validate_connection()
        except LLMError as e:
            raise e
        except Exception as e:
            raise LLMError(f"Error validating connection: {str(e)}")

    def get_model_info(self, model_name: Optional[str] = None) -> Dict[str, Any]:
        """Get information about a model.

        Args:
            model_name: Name of the model. If None, uses the instance model.

        Returns:
            Dictionary with model information.
        """
        try:
            model_to_check = model_name or self.model
            return self.ollama_client.get_model_info(model_to_check)
        except LLMError as e:
            raise e
        except Exception as e:
            raise LLMError(f"Error getting model info: {str(e)}")

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {
            "base_url": self.base_url,
            "model": self.model,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p,
            "top_k": self.top_k,
            "repeat_penalty": self.repeat_penalty,
            "seed": self.seed,
            "keep_alive": self.keep_alive,
            "format": self.format,
        }